# -*- coding: utf-8 -*-
"""Untitled90.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmVKskPBVues_uqX1iynL6ABcaGq6vdb
"""

#Importing the libraries 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import plotly.offline as plotly
# from plotly import iplot
import plotly.graph_objs as graph_objs
from IPython.display import Markdown
def bold(string):
    display(Markdown(string))

#Importing the Dataset 
data = pd.read_csv("netflix_titles.csv")

data

#Converting into date format 
data["date_added"] = pd.to_datetime(data['date_added'])
data['year_added'] = data['date_added'].dt.year
data['month_added'] = data['date_added'].dt.month

data

data.isnull()

data.isnull().sum()

data.describe()

plt.figure(figsize=(15,10))
ax = sns.countplot(x = 'type',data=data,palette = 'viridis')
for p in ax.patches:
        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))

"""There are more movies than TV shows in the dataset """

d1 = data[data["type"] == "TV Show"]
d2 = data[data["type"] == "Movie"]

col = "year_added"

vc1 = d1[col].value_counts().reset_index()
vc1 = vc1.rename(columns = {col : "count", "index" : col})
vc1['percent'] = vc1['count'].apply(lambda x : 100*x/sum(vc1['count']))
vc1 = vc1.sort_values(col)

vc2 = d2[col].value_counts().reset_index()
vc2 = vc2.rename(columns = {col : "count", "index" : col})
vc2['percent'] = vc2['count'].apply(lambda x : 100*x/sum(vc2['count']))
vc2 = vc2.sort_values(col)

trace1 = graph_objs.Scatter(
                    x=vc1[col], 
                    y=vc1["count"], 
                    name="TV Shows", 
                    marker=dict(color = 'rgb(249, 6, 6)',
                             line=dict(color='rgb(0,0,0)',width=1.5)))

trace2 = graph_objs.Scatter(
                    x=vc2[col], 
                    y=vc2["count"], 
                    name="Movies", 
                    marker= dict(color = 'rgb(26, 118, 255)',
                              line=dict(color='rgb(0,0,0)',width=1.5)))
layout = graph_objs.Layout(hovermode= 'closest', title = 'Content added over the years' , xaxis = dict(title = 'Year'), yaxis = dict(title = 'Count'),template= "plotly_dark")
fig = graph_objs.Figure(data = [trace1, trace2], layout=layout)
fig.show()

temp_df = data['rating'].value_counts().reset_index()


# create trace1
trace1 = graph_objs.Bar(
                x = temp_df['index'],
                y = temp_df['rating'],
                marker = dict(color = 'rgb(255,165,0)',
                              line=dict(color='rgb(0,0,0)',width=1.5)))
layout = graph_objs.Layout(template= "plotly_dark",title = 'MOST OF PROGRAMME ON NETFLIX IS TV-14 & TV-MA RATED' , xaxis = dict(title = 'Rating'), yaxis = dict(title = 'Count'))
fig = graph_objs.Figure(data = [trace1], layout = layout)
fig.show()

def pie_plot(cnt_srs, title):
    labels=cnt_srs.index
    values=cnt_srs.values
    trace = graph_objs.Pie(labels=labels, 
                   values=values, 
                   title=title, 
                   hoverinfo='percent+value', 
                   textinfo='percent',
                   textposition='inside',
                   hole=0.7,
                   showlegend=True,
                   marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 14)),
                               line=dict(color='#000000',
                                         width=2),
                              )
                  )
    return trace

plotly.iplot([pie_plot(data['rating'].value_counts(), 'Content Type')])

"""Content Based Movie Filtering """

df2 = data[['title', 'director', 'cast', 'listed_in', 'description']].copy()

df2

!pip install rake-nltk
from rake_nltk import Rake
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

df2.dropna(inplace=True)

blanks = []  # start with an empty list

col=['title','director','cast','listed_in','description']
for i,col in df2.iterrows():  # iterate over the DataFrame
    if type(col)==str:            # avoid NaN values
        if col.isspace():         # test 'review' for whitespace
            blanks.append(i)     # add matching index numbers to the list

df2.drop(blanks, inplace=True)

import nltk
nltk.download('stopwords')
nltk.download('punkt')
# initializing the new column
df2['Key_words'] = ""

for index, row in df2.iterrows():
    description = row['description']
    
    # instantiating Rake, by default it uses english stopwords from NLTK
    # and discards all puntuation characters as well
    r = Rake()

    # extracting the words by passing the text
    r.extract_keywords_from_text(description)

    # getting the dictionary whith key words as keys and their scores as values
    key_words_dict_scores = r.get_word_degrees()
    
    # assigning the key words to the new column for the corresponding movie
    row['Key_words'] = list(key_words_dict_scores.keys())

# dropping the Plot column
df2.drop(columns = ['description'], inplace = True)

# discarding the commas between the actors' full names and getting only the first three names
df2['cast'] = df2['cast'].map(lambda x: x.split(',')[:3])

# putting the genres in a list of words
df2['listed_in'] = df2['listed_in'].map(lambda x: x.lower().split(','))

df2['director'] = df2['director'].map(lambda x: x.split(' '))

# merging together first and last name for each actor and director, so it's considered as one word 
# and there is no mix up between people sharing a first name
for index, row in df2.iterrows():
    row['cast'] = [x.lower().replace(' ','') for x in row['cast']]
    row['director'] = ''.join(row['director']).lower()

df2.set_index('title', inplace = True)
df2.head()

df2['bag_of_words'] = ''
columns = df2.columns
for index, row in df2.iterrows():
    words = ''
    for col in columns:
        if col != 'director':
            words = words + ' '.join(row[col])+ ' '
        else:
            words = words + row[col]+ ' '
    row['bag_of_words'] = words
    
df2.drop(columns = [col for col in df2.columns if col!= 'bag_of_words'], inplace = True)

df2.head()

# instantiating and generating the count matrix
count = CountVectorizer()
count_matrix = count.fit_transform(df2['bag_of_words'])

# creating a Series for the movie titles so they are associated to an ordered numerical
# list I will use later to match the indexes
indices = pd.Series(df2.index)
indices[:5]

# generating the cosine similarity matrix
cosine_sim = cosine_similarity(count_matrix, count_matrix)
cosine_sim

# function that takes in movie title as input and returns the top 10 recommended movies
def recommendations(Title, cosine_sim = cosine_sim):
    
    recommended_movies = []
    
    # gettin the index of the movie that matches the title
    idx = indices[indices == Title].index[0]

    # creating a Series with the similarity scores in descending order
    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)

    # getting the indexes of the 10 most similar movies
    top_10_indexes = list(score_series.iloc[1:21].index)
    
    # populating the list with the titles of the best 10 matching movies
    for i in top_10_indexes:
        recommended_movies.append(list(df2.index)[i])
        
    return recommended_movies

recommendations('Rocky')

recommendations('Bad Boys')

recommendations('Splatter')

recommendations('The Pink Panther')

recommendations('3 Idiots')

recommendations('Jaws')

recommendations('Django Unchained')

recommendations('Spider-Man 3')

