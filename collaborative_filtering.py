# -*- coding: utf-8 -*-
"""Collaborative_Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DT1WhFOwWuEtv1EO-j8kLWukTOZAyHqf

# Load the libraries
"""

# Surprise is an easy-to-use Python scikit for recommender systems. 
# Surprise has a set of built-in algorithms and datasets for you to play with. In its simplest form, it only takes a few lines of code to run a cross-validation procedure
!pip install scikit-surprise
from surprise import Reader, Dataset, SVD, KNNWithMeans, KNNBasic, KNNWithZScore
from surprise import BaselineOnly, CoClustering, SVDpp
from surprise.model_selection import cross_validate
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import plotly.offline as plotly
import sklearn.preprocessing as prep
from sklearn.compose import ColumnTransformer
import scipy.stats as stats
import pylab

"""# Load the dataset"""

data_col_fil = pd.read_csv("ratings.csv")
data_col_fil

"""# Checking if there are any missing values"""

data_col_fil.isnull().values.any()

"""# Summarize the data"""

# Describe the data
data_col_fil.describe()

# Plot the data
data_col_fil.describe().plot()

"""# As we can see above, rating is on a scale of 5. Therefore the data is standardized, and hence it does not require any standardization. Even if we did standardize it again, the results will either be almost the same, or worse.

# However, let us prove it is standardized using a QQ plot.
"""

# QQ plot for rating
stats.probplot(data_col_fil['rating'], dist="norm", plot=pylab)
pylab.show()

"""# This plot shows that the data is standardized because it follows normal line.

# First, using the data as it is, ie. without manual standardization

## Convert datafactor into required data structure
The Reader class is used to parse a file containing ratings.

Such a file is assumed to specify only one rating per line, and each line needs to respect the following structure:

`user ; item ; rating ; [timestamp]`


where the order of the fields and the separator (here ‘;’) may be arbitrarily defined (see below). brackets indicate that the timestamp field is optional.
"""

data = Dataset.load_from_df(data_col_fil[['userId', 'movieId', 'rating']], Reader())
data

"""## Compare the RMSE for different algorithms"""

comparison_table = []

# We are going to try multiple algorithms and compare their results. 
# So, we write a simple for loop that calculates the test RMSE for all algorithms
for algorithm in [SVD(n_epochs=50), KNNBasic(), KNNWithMeans(), KNNWithZScore(), 
                  BaselineOnly(), CoClustering()]:

  # Using RMSE as accuracy measure. You can also use MAE. Cross-validating with 5 folds.
  pred = cross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=False)
  print(algorithm, ': ', pred, '\n')

  # Printing the prediction directly is chaotic and unreadable.
  # Thus we combine it and compare it in a tabular form.
  results = pd.DataFrame.from_dict(pred).mean(axis=0)

  results = results.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))
  comparison_table.append(results)

pd.DataFrame(comparison_table).set_index('Algorithm').sort_values('test_rmse')

"""## From the above, we can see that baselineonly has the best RMSE, followed by SVD and then the rest. We shall train and predict the data with algorithms in the order of their best RMSEs

## Build a training set
"""

trainset = data.build_full_trainset()

"""## Define the prediction models"""

# Tuned the hyperparameters. Default k value gives us better results. 
# SVD however has better results when epochs for it's SGD is increased from 20 to 50, since now the values always fall between 2.3 to 2.6 most times. 
# This is good since user 1's actual rating for movie 31 is 2.5. Thus, our prediction is close enough to allow some buffer while still being accurate.
svd, knnbasic, knnwithmeans, knnwithzscore, baselineonly = SVD(n_epochs=50), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly()

"""## Choosing user 1 for prediction and displaying all their movies and ratings"""

data_col_fil[data_col_fil['userId'] == 1]

"""## Baseline Only"""

baselineonly.fit(trainset)

# Giving userId as 1 and movieId as 31
pred_baselineonly = baselineonly.predict(1, 31)
pred_baselineonly

"""## Simple Value Decomposition"""

svd.fit(trainset)

# Giving userId as 1 and movieId as 31
pred_svd = svd.predict(1, 31)
pred_svd

"""## KNN with Z-Score"""

knnwithzscore.fit(trainset)

# Giving userId as 1 and movieId as 31
pred_knnwithzscore = knnwithzscore.predict(1, 31)
pred_knnwithzscore

"""## KNN with Means"""

knnwithmeans.fit(trainset)

# Giving userId as 1 and movieId as 31
pred_knnwithmeans = knnwithmeans.predict(1, 31)
pred_knnwithmeans

"""## Co-clustering"""

# coclustering.fit(trainset)

# # Giving userId as 1 and movieId as 31
# pred_coclustering = coclustering.predict(1, 31)
# pred_coclustering

"""## Basic KNN"""

knnbasic.fit(trainset)

# Giving userId as 1 and movieId as 31
pred_knnbasic = knnbasic.predict(1, 31)
pred_knnbasic